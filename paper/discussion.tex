\vspace{-1em}
\section {ECN versus Delay}
\label{sec:discuss}

We have now seen that DCQCN and TIMELY (with a small modification) perform
generally well, if properly tuned for a given scenario. Can we thus conclude
that ECN and delay are both "equivalent" signals, when it comes to congestion
control? 

We believe that the answer is no, because ECN has two potential advantages.  One
stems from a small, but important detail about how ECN marking is done on
modern, shared-buffer switches.  The second advantage stems simply from the fact
that ECN marking is done in a ``centralized'' manner, while delay measurements
are done in a distributed manner.

\para{ECN marking is done on packet egress:}
Modern shared-buffer switches, especially those that use Broadcom's merchant
silicon, do ECN marking on {\em packet egress}. When a packet is ready to
depart, the controller checks the egress queue for that port {\em at that
instant}, and depending on the specified marking algorithm (e.g.
Equation~\ref{eq:mark}), decides whether to mark the packet. This means the mark
carried by the packet conveys information about the state of the queue at the
time the packet {\em departs} the queue. 

On the other hand, consider what happens with RTT measurements. If the egress
queue discipline is FIFO within a priority class, (which it typically is), the
delay experienced by a packet reflects the state of the queue at the time the
packet {\em arrives} at the queue. 

What this means is that the control signal carried by the ECN mark is delayed
only by the propagation delay, but the control signal carried by the RTT signal
is affected both by the queuing delay as well as the propagation delay. 

This is a subtle difference: the claim is not that ECN carries more information;
just that the delay of the control loop is decoupled from the queuing delay.

This is why the DCQCN fluid model (Figure~\ref{fig:dcqcn_model}) assumes that
the control loop delay is constant\footnote{DCTCP fluid model
in~\cite{dctcp-analysis} makes the same assumption.}. We cannot make the same
assumption for TIMELY, and  thus we incorporate $\tau'$ and
Equation~\ref{eq:timely_taup} in the TIMELY fluid model
(Figure~\ref{fig:timely_model}).

This means that as the queue length increases (e.g. when there are
more flows), congestion control algorithms that rely on RTT suffer from
increasing {\em lag} in their control loop, making them more difficult to control. We see this
happening for TIMELY (Figure~\ref{fig:timely_stability}). DCQCN is
affected less by  this
effect (Figure~\ref{fig:dcqcn_stability_default}).

As an aside -- we believe that this issue has gone unnoticed in the congestion
control literature so far, because most of it has focused on wide area networks.
In wide area networks, queuing delays and propagation delays can be comparable
(excluding scenarios like buffer bloat~\cite{bufferbloat}). However, in data
center networks, queuing delays can easily dominate switching and
propagation delays.  For example, an Arista 7050QX32 has 40Gbps ports, and a
total shared buffer of switch has 12MB. Even if just 1MB worth of queue builds
up at an egress port\footnote{This requires enabling dynamic thresholding, but it is
almost always enabled in real deployments.}, it takes 200 $\mu$s to drain. In contrast, the one-hop
propagation delay, with cut-through forwarding enabled, is around
1-2 $\mu$s.  Typical diameter of a DC network is 6 hops, so overall propagation
delay is under $20\mu s$.

The reader may argue that it is easy to fix this issue  -- all we have to do is
to build a delay-based protocol that strives to keep the  bottleneck queue
(more-or-less) constant. Then, the control signal delay experienced by the RTT
feedback signal is also fixed, albeit a little higher (propagation delay, plus
fixed queuing delay). 

Unfortunately, it seems to be the case that a delay based congestion control
protocol that maintains a fixed queue, cannot ensure fairness at the fixed point.

\para{Fixed queue comes at the cost of fairness, for delay-based control:}
The key to building protocols that guarantee delay to a fixed quantity is to use
a controller with \emph{integral} control. In the context of network transport
protocols and AQM, integral control was first introduced by the PI
controller~\cite{hollot2001designing}~and REM~\cite{REM}.  The idea behind integral control
is to look at an error signal, e.g. the difference between the actual queue
length and a desired or reference queue length, and adjust the feedback until
the error signal goes to 0. A stable PI controller is guaranteed to achieve
this. In a continuous system, the feedback signal $p(t)$ evolves in the
following way with a PI controller:
\begin{equation}
\small
\frac{dp}{dt} = K_1 \frac{de}{dt}+K_2e(t)
\end{equation}
When the feedback signal converges, both the error signal $e(t)$ as
well as the derivative of the error signal, $de/dt$ must converge to
0. The derivative of the error signal, (the derivative of the queue length), goes to 0
when the sum of the rates $R_i$ match the link capacity $C$. The error signal itself goes to 0
when the queue length matches the reference value. Thus the integral
controller implements the ``match rate, clear buffer'' scheme
presented in~\cite{REM}. 

For DCQCN we can implement the PI controller to mark the packets at the switch
instead of RED (which is a proportional controller without the integral action)
and use that $p$ in the usual way to perform the multiplicative decrease. 

For (patched) TIMELY, we can measure the delay at the end hosts and implement a
PI controller by generating an internal variable ``$p$'', using the error signal
``$e(t)$'' as the difference between the measured delay and some
desired delay. This internal variable $p$ can then replace the $\tfrac{{q(t -
\tau ') - q'}}{{q'}}$ term in Equation (\ref{eq:timely_fixed}) as the feedback
to control the rates.

We implemented the PI controller for both the DCQCN and patched TIMELY fluid
models and performed simulations. As we see in Figure~\ref{fig:dcqcn_pi}~for
DCQCN, all the flows converge to the same (fair) rate and the queue length is stabilized to a preconfigured value, regardless of
the number of flows (as well as regardless of propagation delay). This is
important not only for stability, but also for performance reasons in a data
center networks, where is important to ensure that completion times for short
flows do not suffer from excessive queuing delays~\cite{dctcp}.

\begin{figure}
\subfigure {\includegraphics[width=0.49\columnwidth]{figures/stable_rate_4_pifixed.pdf}}
\subfigure {\includegraphics[width=0.49\columnwidth]{figures/stable_q_4_pifixed.pdf}}
\vspace{-1em}
\caption{DCQCN with PI controller}
\label{fig:dcqcn_pi}
\vspace{-1em}
\end{figure}

In contrast, when we use a PI controller at the end hosts with patched
TIMELY, we see that although we can control the queue
to a specified value (300 KB), we cannot achieve fairness (Figure~\ref{fig:timely_pi}). 
Thus, while patched TIMELY was able to achieve fairness without guaranteeing
delay, patched TIMELY with PI is able to guarantee delay without
achieving fairness.
\begin{figure}
\center
\subfigure[Two flows (7 and 3Gbps)]
{\includegraphics[width=0.49\columnwidth]{figures/timely_withpi_2_4_rate.pdf}}
\subfigure[Two flows (7 and 3Gbps)] {\includegraphics[width=0.49\columnwidth]{figures/timely_withpi_2_4_queue.pdf}}
\subfigure[Ten flows] {\includegraphics[width=0.49\columnwidth]{figures/timely_withpi_10_4_rate.pdf}}
\subfigure[Ten flows] {\includegraphics[width=0.49\columnwidth]{figures/timely_withpit_10_4_queue.pdf}}
\vspace{-1em}
\caption{PI controller to stabilize TIMELY}
\vspace{-1em}
\label{fig:timely_pi}
\end{figure}

We next prove a result that formalizes this fundamental tradeoff between fairness
and guaranteed delay for protocols that rely on delay measurements at the end
points to implement congestion control.

\begin{thm}[Fairness/Delay tradeoff]
\label{thm:fairness-delay}
For congestion control mechanisms that relay purely on end to end
delay measurements, you can either have fairness or a guaranteed delay
bound, but not both simultaneously.
\end{thm}
\begin{proof}
Assume N flows sharing a link of capacity $C$. Then every flow should
distributedly arrive at a rate $C/N$. So the flows need to ``know'' this $N$. If we
use end-to-end delay as the only signal, then this delay has to carry
information about $N$. Hence the converged delay depends on $N$ and cannot be
guaranteed independently. Conversely, if we implement a guaranteed delay
congestion control scheme at the end points, they will converge to any rate
$R_i$ such that $\sum_{i=0}^{N}R_i = C$ making the derivative of measured delay
0 and the actual delay equal to the guarantee. Since the guarantee is set
independent of $N$, the actual delay contains no information on N and such
a scheme cannot ensure fairness.
\end{proof}

%% \begin{figure}[t]
%% \center
%% \includegraphics[width=0.45\textwidth]{figures/design_choice.eps}
%% \vspace{-0.5em}
%% \caption{The design choices and desirable properties.}
%% \vspace{-1.5em}
%% \label{fig:design_choice}
%% \end{figure}

\para{Summary:}
Based on these two factors, {\em i.e.,} the faster feedback to the sources, and
the ability to simultaneously achieve fairness and bounded delay point, we argue
in favor of using ECN instead of delay for congestion control in a data center
environment. %We illustrate this in Figure~\ref{fig:design_choice}.

\para{Practical concerns:}
ECN can require creating per-flow state on the receiver, if ECN marks must be
aggregated. DCQCN~\cite{dcqcn} does this, since RoCEV2 does not send per-packet
ACKs for efficiency reasons. RoCEV2 protocol is implemented on the NIC, which
has limited memory available for caching flow state. Thus, maintaining per-flow
state can cause scalability issues. DCQCN's use of hardware rate limiters on the
sender can also lead to scalability issues, although to a lesser extent.
Detailed discussion of these issues is outside the scope of this paper.

While PI is not implemented in today's commodity switches, as shown
in~\cite{hollot2001designing}~it is a lightweight mechanism that requires less
or comparable computational power as RED, which is already implemented. Also a
variant of the PI controller (PIE) is being used to solve
bufferbloat~\cite{conf/hpsr/PanNPPSBV13,bufferbloat-pi}~, and is part of the
DOCSIS 3.1 standard.

% If we are going to
%modify switches to do extra work, numerous other alternatives like
%XCP~\cite{katabi2002congestion}, RCP~\cite{rcp} and pFabric~\cite{pfabric} that
%offer various other tradeoffs must also be considered. We don't dispute this.
%We are actively working on a fuller exploration of PI and other controllers for
%RDMA -- detailed analysis is beyond the scope of this paper.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
