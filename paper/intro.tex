\section{Introduction}

% background rdma etc.

Large cloud service providers like IBM, Microsoft and Google are increasingly
turning to remote DMA (RDMA) technology, to support applications such as cloud
storage~\cite{erasure-storage}, and distributed memory caches~\cite{farm} that
require high bandwidth and low latency, while minimizing CPU
overhead~\cite{tcp-bolt,dcqcn,timely}.

While there are many ways~\cite{ib-spec, rfc-iwarp,chelsio,rocve2} to enable
RDMA in a data center network, most large cloud service providers enable RDMA
over converged Ethernet (ROCEv2)~\cite{rocev2}, to ensure compatibility with
commodity Ethernet/IP hardware. 

For efficient operation, ROCEv2 needs to eliminate packet drops due to buffer
overflow in switches and routers. This is achieved by enabling Priority Flow
Control (PFC)~\cite{pfc}, and by using a suitable end-to-end congestion control
protocol for individual flows. Recently, a number of congestion control
protocols have been proposed for RDMA networks~\cite{tcp-bolt,dcqcn,timely}.  

% what we do.

In this paper, we analyze the two most recent proposals, namely
DCQCN~\cite{dcqcn}, and TIMELY~\cite{timely} focusing on their stability, rate
of convergence and fairness.

% why we do it.

One of our motivations for analyzing these protocols is their potential for
widespread deployment; given the rising interest in RDMA for high-throughput,
low-delay data center applications. DCQCN is implemented entirely in Mellanox
NICs, and drivers both Windows and Linux operating system are available from
Mellanox. Thus, anyone can deploy DCQCN in their data centers. While Google's
implementation of TIMELY is not publically available, the core timing
functionality it relies on is supported by most Mellanox NICs and their drivers. 

However, apart from this practical consideration, we are also driven by
scientific curiosity. DCQCN relies on ECN marking by switches to detect
congestion, while TIMELY uses changes in RTT as a congestion signal. This
naturally raises the question of how the performance of these two mechanisms
compares under high-speed, low-latency, drop-free data center networks.

% summary of difference from prior work.

While there has been a lot of prior work on design and analysis of ECN and
delay-based congestion control~\cite{ecn, tcp-vegas, Hollot:PIController}, with few
recent exceptions~\cite{dctcp-analysis, qcn-analysis} it was primarily done in
the context of wide area networks, and for TCP-like window-based congestion
control algorithms.  We also note that neither ECN-based, nor delay-based
congestion control has seen widespread deployment in the wide area Internet, due
a host of reasons. We are now seeing the first large-scale deployment of these
mechanisms, in a very different kind of environment - namely, the high-speed,
low-latency, drop-free (due to PFC~\cite{pfc}) data center networks, so this is
a ripe time for analytical exploration of these protocols.

% Methodology.

As in~\cite{dctcp-analysis, qcn-analysis}, we analyze DCQCN and TIMELY using
detailed fluid models and NS3 simulations. Fluid models are useful for analyzing
properties such as stability, and for rapid exploration of parameter space.
Simulations are useful for studying
packet-level dynamics and establishing the fidelity of the fluid model
to the real system. For DCQCN, our starting point for the analysis is
the fluid model
proposed by the authors and is known to be a good fit for
actual hardware implementation~\cite{dcqcn}. For TIMELY, we develop
our own fluid model based on the paper description and since we do not have access to
TIMELY implementation, we relied on extensive discussion with authors
of~\cite{timely} to ensure that our fluid and simulation models are good
representations of the protocol. Our NS3 simulation code, as well as Matlab code
used to solve the fluid models will be publically avaiable.

% summary of contributions and findings.

The key contributions, and findings of this paper are as follows.

\para{DCQCN:} $(i)$ We extend the fluid model proposed in~\cite{dcqcn}. We
analytically show that DCQCN has a unique fixed point, where all flows get their
fair share. $(ii)$ We show that while DCQCN is generally stable around this
fixed point, as long as the feedback latency is low. The relationship between
stability and the number of competing flows is non-monotonic, which is very
different from TCP's behavior~\cite{misra:TAC2002}. DCQCN is stable for
both very small, and very large number of flows, and tends to be unstable
inbetween; especially if the feedback latency is high.  $(iii)$ We propose new
parameter values to enhance DCQCN's stability, and we derive new parameter
values for higher bandwidth (100Gbps) links. $(iv)$ Using simulations, we
explore the convergence properties of DCQCN, and find that \fixme{fill in}.

\para{TIMELY:} $(i)$ We develop a fluid model for TIMELY. Using NS3 simulations,
we show that the fluid model is accurate. $(ii)$ simple analysis of the fluid model
reveals that TIMELY can have infinite fixed points -- and thus can have
arbitrary unfairness. We show that this behavior springs from the fact that
TIMELY uses changes in RTT \textit{gradient} as a congestion signal, \fixme{as opposed to RTT itself}.
Our simulations confirm the analysis. $(iii)$ Using NS3
simulations, we analyze TIMELY's behavior in further detail, revealing that
TIMELY's decision to not use hardware rate limiters is a double-edged sword that
can either help or hurt the performance, depending on the scenario. $(iv)$ We
propose a small fix to TIMELY that alleviates the stability problem. We verify
our fix via simulations.

Leveraging the insights from analysis of each protocol, we finally provide a
general comparison of the two approaches to the congestion control. \fixme{We find that... (can we make claims about about performance overall, algorithm simplicity, congestion signal ECN vs delay).}


